---
title: "MA710 Assignment4 - Decision Tree Classification and K-nearest Neighbors Classification"
author: "Xiang Li, Xue Zhou"
date: "4/24/2017"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message = FALSE, eval=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```

# Table of Contents
* 1 [Introduction](#Introduction) 
* 2 [Data Preparation](#2)
    * 2.1 [Load the Data](#2.1)
    * 2.2 [Testing and Training Data Set](#2.2)
* 3 [Decision Tree Classification](#3)
* 4 [k-Nearest Neighbour Classification](#4)
* 5 [Conclusion](#conclusion)
* 6 [Future Studies](#futurestudies)   

First we loaded the required packages as the code block below. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#desicion tree
library(rpart)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(party)
library(partykit)
library(caret)
library(dplyr) #missing values
#knn
library(kknn)
library(caret)
library(e1071)
options(dplyr.width=Inf) 
```


# 1 Introduction<a id="Introduction"></a>

In this analysis, we are going to use **Decision Tree** classification and **K-nearest Neighbours** classification to classify the ```Control_factor``` variable (which has three levels: private for-profit, private non-profit and public ) in the College Scorecard Data Set. Since we can get the control type information of a school by simply goggling, the goal of the analysis is to profile the institutions of each control type rather than to predict which control type a school is.


# 2 Data Preparation<a id="2"></a>

Prior to the analysis, we need to prepare the data. Getting a clean data set ready for Decision Tree Modeling and KNN Analysis requires us to remove unnecessary columns, deal with missing values, and split data into training and testing sets.

## 2.1 Load the dataset<a id="2.1"></a>

We load the data set and take a look at the structure of data.

```{r}
setwd("~/Desktop/ma710-a4")
data_import = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
str(data_import)
```

The original data set has 7793 observations and 19 variables. 


First of all, we removed useless columns and renamed the row names as University ID. Then we dropped all the instances with missing values.

```{r}
#get rid of ID, university name and state columns, rename the rownames as the university ID. 
data.with.rownames <- data.frame(data_import[,-c(1:4)], row.names=data_import[,2])

data_nomissing <- na.omit(data.with.rownames)

str(data_nomissing)  #4528 * 15
```

The new data set ```data_nomissing``` has 4528 rows and 15 variables.


## 2.2 Testing and Training<a id="2.2"></a>

After obtaining a clean data set, let's split the complete data into training and testing sets. The ratio of training sample size to testing sample size is **2:1**. Also, we exported the training and testing set to two separate csv files for easier access in the future.

```{r}
set.seed(2) #reproduce the results

# Store row numbers for training set: index_train
index_train <- sample(1:nrow(data_nomissing), 2 / 3 * nrow(data_nomissing))

# Create training set: data_training
data_training <- data_nomissing[index_train, ]

write.csv(data_training, file = "Data_training.csv") 

# Create test set: data_testing
data_testing <- data_nomissing[-index_train, ]

write.csv(data_testing, file = "Data_testing.csv")
```

The file ```Data_testing.csv``` and ```Data_training.csv``` are ready for further references. 


With the code block below, we started decision tree analysis by retrieving the training and testing data.

```{r}

data_d = read.csv("Data_training.csv",
                   header = TRUE, na.strings = 'NA')#includes the row.names, just for testing
data_d2 = read.csv("Data_testing.csv",
                   header = TRUE, na.strings = 'NA')


data_training <- data.frame(data_d[,-1], row.names=data_d[,1])
str(data_training)
summary(data_training)

data_testing <- data.frame(data_d2[,-1], row.names=data_d2[,1])
```

There are 3018 rows and 15 variables in the training set, and 1510 rows and 15 variables in the testing set.


Before implementing the decision tree, it is necessary to look at the frequency of the response variable ```Control_factor``` to see if there exists an unbalanced data problem. If a class of the response variable is extremely predominate, the prediction of the model would always tend to be that class, which may influence the predictive power of the model. 

```{r}

table(data_training$Control_factor) 
prop.table(table(data_training$Control_factor))#check the existence of unbalanced data problem, 1387 vs 700 vs 931
```

From the table above, we see that 46.0% of schools belong to private for-profit institutions, 23.2% of schools are private non-profit and 30.8% of schools are public. As we can tell, the data set doesn't have a serious unbalanced data problem.


# 3. Decision Tree Classification<a id="3"></a>

We started to grow a full tree without any constrains to classify the ```Control_factor``` variable. We also plot a simple version of classification tree with the ```plot``` function.

```{r}
set.seed(2)
original_tree <- rpart(Control_factor ~ ., method = "class",
                          data = data_training,control = rpart.control(cp=0,minsplit = 0))

plot(original_tree)

```

Based on the tree plot, the original tree is very large and messy, which is subject to an overfitting problem and may lead to poor generalization to unseen data. So we want to alleviate the overfitting problem by adjusting some parameters of the model.


We used **Grid Search** to help up find the best ```cp``` and ```minsplit``` that can yield the highest model accuracy. ```cp``` parameter helps to adjust model complexity and ```minsplit``` determines the minimum number of observations that must exist in a node, in order for a split to be attempted. 

Considering that the Grid Search for ```rpart```  is only able to tune the ```cp``` parameter, we would like to find the best `cp` value for a set of ```minsplit``` values: **30**, **50** and **80**, and then compare accuracy of these three models to get the best parameter combination.

```{r}
set.seed(42) 
# prepare training scheme, create a control object to control how the train function creates the model
control <- trainControl(method="repeatedcv", number=5, repeats = 3)

# design the parameter tuning grid 
grid <- expand.grid(cp = c(0.001,0.003,0.01))

set.seed(42) 

# train the model
model1 <- train(Control_factor~., data=data_training, method="rpart", 
                trControl=control, 
                tuneGrid=grid,
                minsplit=30)  #additional parameter
model1 

model2 <- train(Control_factor~., data=data_training, method="rpart", 
                
                trControl=control, 
                tuneGrid=grid,
                 minsplit=50)
model2

model3 <- train(Control_factor~., data=data_training, method="rpart", 
                trControl=control, 
                tuneGrid=grid, 
                minsplit=80)
model3


```

With the above code block, the ```model1``` object lists the Accuracy values for three levels of **0.001**, **0.003**, **0.010**, given the ```minsplit``` is 30;  the ```model2``` object lists the Accuracy values for three levels of **0.001**, **0.003**, **0.010**, given the ```minsplit``` is 50;  the ```model3``` object lists the Accuracy values for three levels of **0.001**, **0.003**, **0.010**, given the ```minsplit``` is 80; 


Then we selected the parameter combination which lead to the highest Accuracy value for each CP value.

```{r}
#best for minsplit = 20
model1$results[which.max(model1$results[,c('Accuracy')]),c('cp','Accuracy')]

#best for minsplit =50
model2$results[which.max(model2$results[,c('Accuracy')]),c('cp','Accuracy')]

#best for minsplit = 80
model3$results[which.max(model3$results[,c('Accuracy')]),c('cp','Accuracy')]
```

Based on the output, we can see that the model with minsplit = 50 and cp = 0.001 gives the highest model accuracy: 0.8855756. 


Then we rebuilt the tree with the control parameter: ```cp = 0.001 ``` and ```minsplit = 50```. A tree plot was also plotted with ```fancyRpartPlot``` function.

```{r}
tree_tuned <- rpart(Control_factor ~ ., method = "class",
                          data = data_training,control = rpart.control(
                          cp=0.001,
                          minsplit = 50))
fancyRpartPlot(tree_tuned)

```

Based on the tree plot, the new tree has a more reasonable size than the original tree. However, the new tree is still complicated, which may contain overfitting issues. We used the cross validation error to prune back the tree to avoid overfitting the data. 


The ```printcp``` function allows us to select a tree size that minimizes the cross-validated error.  After the tree size is determined, we can prune the tree with the corresponding **cp** value and generate a new tree with ```prune``` function. 

```{r}

printcp(tree_tuned)
bestcp <- tree_tuned$cptable[which.min(tree_tuned$cptable[,"xerror"]),
                                 "CP"]
tree.pruned <- prune(tree_tuned, cp = bestcp)

fancyRpartPlot(tree.pruned)


```

From the CP table, we can see that split = 13 yields the lowest cross-validation error 0.22931,  the corresponding CP value is 0.0026569, and the pruned tree is saved in ```tree.prunned```. Based on the tree plot, the ```tree.prunned``` has 18 terminal nodes, which is still too large to interpret and visualize. 


Then we used the ```maxdepth``` parameter to adjust the tree size, limiting the tree level to only 3. 

```{r}

tree.pruned.limited <- rpart(Control_factor ~ ., method = "class",
                          data = data_training,control = rpart.control(
                          cp=0.001,
                          minsplit = 50, 
                          maxdepth = 3))

fancyRpartPlot(tree.pruned.limited, sub = 'Decision Tree Plot', palettes=c("Greys", "Oranges","Blues"))

```

Based on the tree plot, the final tree model has only 8 terminal nodes, which is in a reasonable size. After having an understanding of how the tree look like, we also want to evaluate how well the tree performs in terms of the predictive accuracy. The following sections compared the values predicted from the final model with the true values for both training data and testing data.


We used ```tree.pruned.limited``` model to generate predictions on the training set and check the model accuracy with a confusion matrix.

```{r}
train_preds = predict (tree.pruned.limited,
                       newdata = data_training,
                       type="class")
confusionMatrix(train_preds, data_training$Control_factor)


```

The diagonal of the Confusion Matrix indicates the instances that are correctly classified in the training data set. We see that for training data set, 2596 instances are classified correctly, and the model has an accuracy of 0.8602 on the training data set. 


As it is usual for the predictive model to perform well on the data set used to create it, Let's use the model to make predictions on the test data set to see how well it performs on the unseen data. 

```{r}
test_preds = predict (tree.pruned.limited,
                       newdata = data_testing,
                       type="class")
confusionMatrix(test_preds, data_testing$Control_factor)

```

We see that for the testing data set, 1278 instances are classified correctly, and the model has an accuracy of 0.8464 on the training data set. The model has a slightly better predictive power on the training data set than the testing data set, which is within our expectation. As a conclusion, the model performs very well on both training data and testing data, and has a reasonable tree size. Thus the ```tree.pruned.limited``` is considered as the final decision tree model. 


With the following code block, we interpreted the final model by plotting the final tree and requesting the summary statistics. 

```{r}

rpart.plot(tree.pruned.limited, sub = 'Decision Tree Plot')

print(tree.pruned.limited)
```

From the plot, we can tell ```DEP_STAT_PCT_IND```(Percentage of students who are financially independent) is the most important variable to predict the control type of a school, ```NPT4_COMBINE```(average net price for TITLE IV institutions of both public and private.), ```MD_FAMINC ```(Median family income in real 2015 dollars), ``` MD_EARN_WNE_P10```(Median income ten years after entry) and ``` APPL_SCH_PCT_GE2 ```(Percentage of students submitting 2 or more than 2 applications) are also the important variables. 

As seen from the plot, in each node shows:
 1. The predicted class (Private For-profit, Private Nonprofit and Public) 
 2. The predicted probability of each class
 3. The percentage of observations in this node
 
Below are the 8 rules concluded from the final tree model: 

*Rule 1*: 

if an school has Percentage of students who are financially independent >= 0.59, average net price for TITLE IV institutions of both public and private >= 10752.5, Median family income in real 2015 dollars < 24511.41, then this school is classified as the control type of **Private For-profit**
        
*Rule 2*: 

if an school has Percentage of students who are financially independent >= 0.59, average net price for TITLE IV institutions of both public and private >=10752.5, Median income ten years after entry < 21400, then this school is classified as the control type of **Private For-profit**
        
*Rule 3*: 

if an school has Percentage of students who are financially independent < 0.59, average net price for TITLE IV institutions of both public and private >=14558, Percentage of students submitting 2 or more than 2 applications < 0.49, then this school is classified as the control type of **Private For-profit**
        
*Rule 4*: 

if an school has Percentage of students who are financially independent < 0.59, average net price for TITLE IV institutions of both public and private < 14558, Median income ten years after entry < 24350, then this school is classified as the control type of **Private For-profit**

*Rule 5*: 

if an school has  Percentage of students who are financially independent >= 0.59, average net price for TITLE IV institutions of both public and private >= 10752.5, Median family income in real 2015 dollars >= 24511.41, then this school is classified as the control type of **Private Non-profit**          

*Rule 6*: 

if an school has  Percentage of students who are financially independent < 0.59, average net price for TITLE IV institutions of both public and private >=14558, Percentage of students submitting 2 or more than 2 applications >= 0.49, then this school is classified as the control type of **Private Non-profit**

*Rule 7*: 

if an school has  Percentage of students who are financially independent >= 0.59, average net price for TITLE IV institutions of both public and private < 10752.5, Median family income in real 2015 dollars >= 24511.41,then this school is classified as the control type of **Public**  
      
*Rule 8*: 

if an school has  Percentage of students who are financially independent < 0.59, average net price for TITLE IV institutions of both public and private < 14558, Median income ten years after entry >= 24350, then this school is classified as the control type of **Public**

Based on the rules generated from the decision tree model, we can create a profile for each control type as below. 

**Private For-profit**: 


**Public**:


**Private Non-profit**:


In the following part, we used another algorithm - K-nearest Neighbor to classify a school's control type. Then we compared the performance of two final models to compare which model performs better in terms of classification ability.


# 4. k-Nearest Neighbour Classification<a id="4"></a>

First of all, we imported the training and testing data we created in the 1st part, and removed the unnecessary column from the data set. To better compare the model performance of decision tree and KNN, the training and testing data sets we use here are identical to the data sets we used for building the decision tree. 

```{r}
#training
data_k_d= read.csv("Data_training.csv",
                   header = TRUE, na.strings = 'NA')
data_training_k <- data.frame(data_k_d[,-1])


#testing 
data_k_d2 = read.csv("Data_testing.csv",
                   header = TRUE, na.strings = 'NA')
data_testing_k = data.frame(data_k_d2[,-1])
```

Accordingly, ```data_training_k``` and ```data_testing_k``` are the resulting data sets. 


Then we encoded the factor variables into dummy variables for calculating the distance metrics between each pair of instances when implementing KNN. In our data set, only one factor variable ```PREDDEG_factor``` (Predominant Degree) needs to be converted. We created a function ```convertNum()``` to automate the entire conversion process. 

```{r}
convertNum = function(mydata) {
    mydata = mydata[,-1]
    dummy_preddeg = model.matrix(~PREDDEG_factor-1,mydata)
   
    colnames(dummy_preddeg) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
    
    data_combine = cbind(mydata, dummy_preddeg) 
    return (data.frame(data_combine[,-1]))  #get rid of the converted col PRED
}
```

The function encodes the factor variable ```PREDDEG_factor``` into dummies, renames the new columns and drops the original factor variable after the new data set is integrated. 


Then we applied the function to the training and testing data to perform data transformation.

```{r}
data_knn_training = convertNum(data_k_d)
data_knn_testing = convertNum(data_k_d2)

```

```data_knn_training``` and ```data_knn_testing ``` are the resulting data sets. 


Next, we used KNN algorithm to build a KNN model. ```train.kknn``` performs leave-one-out cross validation and is computationally very efficient. We set kmax = 9 so the number of nearest neighbor from 1 to 9 will be considered. The ```train.kknn``` function is also able to find out the best kernal among among ```triangular```,```rectangular```,```epanechnikov``` and ```optimal```. We set ```scale``` to TRUE to ensure every variable receives an equal weight. With the ```plot``` function, we can get a plot to show the performance of model with each kernel for different k values. 

```{r}
model <- train.kknn(Control_factor ~ ., 
                    data = data_knn_training, 
                    kmax = 9,
                    scale = TRUE,
                    kernel = c("triangular", "rectangular", "epanechnikov", "optimal"))

plot(model)
model #display the summary
```

From the result we see that the when k=4, the model generates the best result - 7.5% classifications rate. Also, the best kernel is **rectangular kernel**, which is also can be concluded from the plot. Moreover, the plot also indicates that a optimal kernel with k = 5,6,8 can also generate a model with low misclassification rates. 


After the model is built, we need to evaluate how the model performs on both training and testing data. With the ```confusionMatrix``` function, we can get 
the confusion matrix for classification and the model performance for each class.

```{r}
#training error
prediction_training <- predict(model, data_knn_training[, -1])
confusionMatrix(reference=data_knn_training[, 1], data=prediction_training )

```

The overall accuracy of the model is 95%, which indicates that the model has a very good performance on the training data set. Looking at the Sensitivity and Specificity for each class, we can also tell the model has a good prediction power given the high Sensitivity value and high Specificity value. 


Then we predict the model on the testing data set. 

```{r}
#testing
prediction_testing  <- predict(model, data_knn_testing[, -1])
confusionMatrix(reference=data_knn_testing[, 1], data=prediction_testing)

```

We see that the overall accuracy of the model on testing set is 92%, slightly lower than on the training data set.
The Sensitivity and Specificity for each class is lower than training data set, but is still falling into the acceptable range. As a conclusion, the KNN final model performs well on both training and testing data. 


# 5. Conclusions <a id="conclusion"></a>

In the above analysis, we used Decision Tree model and K-nearest Neighbor model to classify the Control Type (```control_factor```) variable. 

The decision tree model gives an accuracy score of 0.8602 on the training data set and an accuracy score of 0.8464 on the testing data set while the KNN model gives an accuracy score of 0.95 on the training data set and an accuracy score of 0.92 on the testing data set.  Overally speaking, KNN model yields a better accuracy than we achieved with the tree-based model, but the two final models both have a very good performance, which suggests that the existing features are able to distinguish one type of school from another very effectively.

Two models have their own strengths and weakness. Decision tree is one of the "Eager Learners", which are designed to learn a model that maps the input attributes as soon as the training data set is available: they first build a classification model on the training data set before being able to actually classify an unseen observation from test data set. KNN is one of the "Lazy Learners", the classifier delays the process of modeling the training data set until it is needed to classify the test examples: it does not build any classification model. It directly learns from the training observations and starts processing data only after it is given a test observation to classify. 

Additionally, Decision Tree is very easy to interpret while KNN outperforms decision tree in terms of the prediction power. Therefore, it's better to use KNN model for prediction rather than classification. 

To sum up, the decision tree model outperforms KNN in terms of the classification, while KNN is a better choice when used for prediction. Moreover, decision tree is easier to understand, especially for interpretation and presentation. 

# 6. Future Studies <a id="futurestudies"></a>

One question we would like to address in the future is to find a better algorithms to perform parameters tuning. Although there are many tunable parameters in decision tree analysis, the Grid Search algorithm in ```caret``` package can only allow us to tune  the ```cp``` parameter. In Python, Grid Search allows us to test a combination of many parameters at the same time when building the decision tree, thus it would be helpful if we can find a counterpart in R.

Additionally, other classification algorithms like **Random Forest**, **Neural Network** and **Support Vector Machine** are also worth trying, which allows us to explore other potential models with better performances.
