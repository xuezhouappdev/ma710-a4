---
title: "MA710 Assignment4 - Decision Tree Classification and K-nearest Neighbors Classification"
author: "Xiang Li, Xue Zhou"
date: "4/24/2017"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message = FALSE, eval=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```

# Table of Contents
* 1 [Introduction](#Introduction) 
* 2 [Data Preparation](#2)
    * 2.1 [Load the data](#2.1)
    * 2.2 [Testing and Training](#2.2)
* 3 [Decision Tree Classification](#3)
* 4 [k-Nearest Neighbour Classification](#4)
* 5 [Conclusion](#conclusion)
* 6 [Future Studies](#futurestudies)   

Load the required package.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#desicion tree
library(rpart)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(party)
library(partykit)
library(caret)

#missing values
library(dplyr)

#knn
library(kknn)
library(caret)
library(e1071)
options(dplyr.width=Inf) 
```


# 1 Introduction<a id="Introduction"></a>

In this analysis, we are going to use Decision Tree Model and K-nearest Neighbours Model to classify the Control variable (which has three levels: private for-profit, private non-profit and public ) in the College Scorecard Data Set. Since we can get the control type info of a school by simply googling, the goal of the analysis is to profile the institutions of each control type rather than to predict which control type a school  belongs to.


# 2 Data Preparation<a id="2"></a>

Prior to the analysis, we need to prepare the data. Getting a clean data set ready for Decision Tree Modeling and KNN Analysis requires us to remove unnecessary columns, deal with missing values, and split data into training and testing set.

## 2.1 Load the dataset<a id="2.1"></a>

We load the data set and take a look at the structure of data.

```{r}
data_import = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
str(data_import)
```

First of all, we remove useless columns and rename the row names as University ID. Then we drop all the instances with missing values.

```{r}
#get rid of ID, university name and state columns, rename the rownames as the university ID. 
data.with.rownames <- data.frame(data_import[,-c(1:4)], row.names=data_import[,2])

data_nomissing <- na.omit(data.with.rownames)

str(data_nomissing)  #4528 * 15
```

Now the data set has 4528 rows and 15 variables.


## 2.2 Testing and Training<a id="2.2"></a>

After obtaining a clean data set, let's split the complete data into training and testing set. The size of training and testing set has a ratio 2/3:1/3. Also, we export the training and testing set to two separate csv files for easier access in the future.

```{r}
set.seed(2)

# Store row numbers for training set: index_train
index_train <- sample(1:nrow(data_nomissing), 2 / 3 * nrow(data_nomissing))

# Create training set: data_training
data_training <- data_nomissing[index_train, ]

write.csv(data_training, file = "Data_training.csv") 

# Create test set: data_testing
data_testing <- data_nomissing[-index_train, ]

write.csv(data_testing, file = "Data_testing.csv")
```

So now we can simply read the training and testing set without running preparation code every time.

```{r}

data_d = read.csv("Data_training.csv",
                   header = TRUE, na.strings = 'NA')#includes the row.names, just for testing
data_d2 = read.csv("Data_testing.csv",
                   header = TRUE, na.strings = 'NA')


data_training <- data.frame(data_d[,-1], row.names=data_d[,1])

data_testing <- data.frame(data_d2[,-1], row.names=data_d2[,1])
```

We see that there are 3018 rows and 15 variables in the training set. There are 1510 rows and 15 variables in the testing set.

Let's take a look at the response variable to see if the data is biased. If a class of the response variable is extremely predomoninat, the prediction of the model would always tend to be that class, which may influence the predictive power of the model. 

```{r}
levels(data_training$Control_factor)
table(data_training$Control_factor) 
prop.table(table(data_training$Control_factor))#check the existence of unbalanced data problem, 1391 vs 718 vs 909
```

From the table above, we see that 46% of schools belong to private for-profit institutions, 23.2% of schools are private non-profit and 30.8% of schools are public. As we can tell, the data set doesn't have a serious bias problem.

#3. Decision Tree Classification<a id="3"></a>

Next, we start building a decision tree to classify the Control variable. 

```{r}
set.seed(2)
original_tree <- rpart(Control_factor ~ ., method = "class",
                          data = data_training,control = rpart.control(cp=0,minsplit = 0))

# can change the control to try, cp defaul 0.05;  different results 
```

The resulting tree is very large and subject to overfitting problem. So we want to alleviate the overfitting problem by controlling some parameters of the model.

We use Grid Search to help up find the best ```cp``` and ```minsplit``` that can yield the highest accuracy. For some reason, the implementation of Grid Search in R is only able to look for best ```cp``` parameter. Therefore we find the best `cp` number for each of a set of three ```minsplit``` numbers, and then compare accuracy of these three models to get the best parameter combination.

```{r}

# prepare training scheme
control <- trainControl(method="repeatedcv", number=5)
# design the parameter tuning grid
grid <- expand.grid(cp = c(0.001,0.003,0.01))

# train the model
#minsplit =30
set.seed(42)

model1 <- train(Control_factor~., data=data_training, method="rpart", metric = 'Accuracy', trControl=control, tuneGrid=grid, minsplit=20)


model2 <- train(Control_factor~., data=data_training, method="rpart", trControl=control, tuneGrid=grid, minsplit=50)


model3 <- train(Control_factor~., data=data_training, method="rpart", trControl=control, tuneGrid=grid, minsplit=80)
```


Since we've found the best cp value for minsplit = 20, minsplit = 50 and minsplit = 80, let's compare the accuracy of these three combination and choose the best parameters.

```{r}
#best for minsplit = 20
model1$results[which.max(model1$results[,c('Accuracy')]),c('cp','Accuracy')]

#best for minsplit =50
model2$results[which.max(model2$results[,c('Accuracy')]),c('cp','Accuracy')]

#best for minsplit = 80
model3$results[which.max(model3$results[,c('Accuracy')]),c('cp','Accuracy')]
```

We can see that the model with minsplit = 80 and cp = 0.003 gives the best result. After finding the best parameters, we also would like to prune the tree by looking at cross-validation error.

```{r}
tree1 <- rpart(Control_factor ~ ., method = "class",
                          data = data_training,control = rpart.control(cp=0.003,minsplit = 80))

tree1$cptable
```

We can see that nsplit = 13 has the lowest cross-validation error, after that the error starts to increase. So we set this as a cut-off point.

```{r}
bestcp <- tree1$cptable[which.min(tree1$cptable[,"xerror"]),
                                 "CP"]

tree1.pruned <- prune(tree1, cp = bestcp)
prp(tree1.pruned)
```

The tree is the best model we use for prediction. But we can see from the plot that this is a large tree with many nodes. Let's try setting the ```maxdepth``` so that we will be able to visualize the top levels of the tree.

```{r}
tree2 <- rpart(Control_factor ~ ., method = "class",
                          data = data_training,control = rpart.control(cp=0.001,minsplit = 40, maxdepth = 3))

fancyRpartPlot(tree2, sub = 'Decision Tree Plot') #pretty one 
```

From the plot, we can tell Predominant Degree, Net Price (Net living cost), Financially Independent rate and Three-year Repayment rate are important variables to classify Control type. For each Control, we can create a profile for them.

Private for-profit: Predominant Degree is associate degree or certificate degree an with a Net Price greater than $9,949   *or*
Predominant Degree is bachelor degree or graduate degree with a Three-year Repayment Rate less than 36% and percentage of financially indenpendent student more than 61%.

Public School:Predominant Degree is associate or certificate with net price less than $9,949  *or*

Predominant Degree is bachelor degree or graduate degree
with three-year repayment rate over 36% and net price less than $15,000 


Private non-profit:
Predominant Degree is bachelor or graduate degree with a Three-year Repayment rate greater than 36% and Net Price greater than $15,000 *or*

Predominant Degree is bachelor or graduate degree, Three-year Repayment rate less than 36%, percentage of financially independent students less than 61%.


Then we see how the cross-validation error decreases as the size of tree increases.

```{r}
plotcp(tree2) #visualize the cross-validation results 
```

Plotting the size of tree against cross-validation error, we see that that as the size of tree increases, the cross-validation error decrease and level off after depth is greater than 7.

After having an understanding of how the tree look like, we also want to evaluate how well the tree performs in terms of prediction. The following code compare the predicted values against the true values on both training data and testing data.

The following code generates the confusion matrix for training and testing data.

```{r}
#training error
pred_prune_train <- predict(tree1.pruned, newdata = data_training,  type = "class")
#testing error 
pred_prune_test <- predict(tree1.pruned, newdata = data_testing,  type = "class")

#matrix table of the original tree - train 
conf_matrix_train = table(data_training$Control_factor, pred_prune_train)

#matrix table of the original tree
conf_matrix_test = table(data_testing$Control_factor, pred_prune_test)
```

Let's take a look at the confusion matrices.

```{r}
conf_matrix_train
conf_matrix_test
```

The diagonal of the confusion matrix indicates the instances that are correctly classified. We see that for training and testing data, most instances are classified correctly.

We also calculate the overall accuracy.

```{r}
#for training data
acc_prune_training<- sum(diag(conf_matrix_train))/nrow(data_training)
acc_prune_training

#for testing data
acc_prune<- sum(diag(conf_matrix_test)) / nrow(data_testing)
acc_prune
```

We see that the performance on training set is slightly better than testing set, which makes sense. Overall, the model performance is good.
In the following part, we are going to use another model - K-nearest Neighbour model. Then we will compare the performance of two models to see which model performs better in terms of classfiying the Control type.





# 4. k-Nearest Neighbour Classification<a id="4"></a>

First of all, we read the training and testing data we create in last part.

```{r}
data_k_d= read.csv("Data_training.csv",
                   header = TRUE, na.strings = 'NA')#includes the row.names, just for testing
data_k_d2 = read.csv("Data_testing.csv",
                   header = TRUE, na.strings = 'NA')

str(data_k_d2 )
str(data_k_d)
```

After taking a look at the data, we notice that first column is useless so we remove it.

```{r}
data_training_k <- data.frame(data_k_d[,-1])

data_testing_k = data.frame(data_k_d2[,-1])
```

Then we would like to encode numerical variables into dummies since we need calculate distance between each pair of instances when using KNN. In order to calculate the distance, we need all variables to be numerical. We create a function to automate the entire preparation process. The function encodes the Predominant Degree into dummies, renames the new columns and finally drop the original Prefominat Degree variables.


```{r}
convertNum = function(mydata) {
    mydata = mydata[,-1]
    dummy_preddeg = model.matrix(~PREDDEG_factor-1,mydata)
   
    colnames(dummy_preddeg) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
    
    data_combine = cbind(mydata, dummy_preddeg) 
    return (data.frame(data_combine[,-1]))  #get rid of the converted col PRED
}
```

After the function is created, we apply the function to training and testing data to perform data transformation.

```{r}

data_knn_training = convertNum(data_k_d)
data_knn_testing = convertNum(data_k_d2)
```

Notice that the Predominat Degree variable in testing data doesn't have a NotClasified level, so we want to add this columns and set all cells to zero so that the model built on training data can be applied on testing data diretly later.

```{r} 
data_knn_testing$NotClassified  =0
```

Next, we use KNN algorithm to build a model. We set kmax = 9 so the number of nearest neighbor from 1 to 9 will be considered. We also would like to find out which kernal is the best among ```triangular```,```rectangular```,```epanechnikov```,```optimal```.    

```{r}
model <- train.kknn(Control_factor ~ ., 
                    data = data_knn_training, 
                    kmax = 9,
                    scale = TRUE,
                    kernel = c("triangular", "rectangular", "epanechnikov", "optimal"))

model
```

From the result we see that when we choose k=4, the model generates the best result - 7.5% misclassification rate. Also, the best kernel is rectangular kernel.

Then we create a plot to show the performance of model with each kernel for different k values. 

```{r}
plot(model)

prediction <- predict(model, data_knn_testing[, -1])
```

The red triangle indicates the best result and we also see that a optimal kernel with k = 5,6,8 also produce good results.

After the model is built, we need to evaluate how the model performs on both training and testing data.

```{r}
#training error
prediction_training <- predict(model, data_knn_training[, -1])
confusionMatrix(reference=data_knn_training[, 1], data=prediction_training )
```

```confusionMatrix```shows the confusion matrix for classification and the model performance for each class. The overall accuracy of the model is 95%, which indicates a very good performance. Looking at each class, we can also tell the model performance on each class is also desirable while the sensitivity is slighly lower. A lower sensitivity rate suggests that the model is relative worse at identifying TRUE private non-profit schools.

Then we evaluate the model on testing data.

```{r}
confusionMatrix(reference=data_knn_testing[, 1], data=prediction)
```

We see that the overall accuracy of the model on testing set is 92%, slighly worse than on the training set. Looking at each class, we also see the sensitivity rate for private non-profit schools is worse than other metrics, which suggests that we may want to collect some new features that can help identify the TRUE private non-profit schools.


#5 Conclusion]<a id="conclusion"></a>

In the above analysis, we use Decision Tree Model and K-nearest Neighbour Model to classify the Control variables. Two models we build both have a very good performance, which suggests that the available features are able to distinguish one type of school of each other very effectively.

Two models have their own strengths and weakness. Decision Tree Model is very easy to interprete. From the tree plot we created, we know that .......... are important factors to classify Control type.

On the other hand, Decision Tree model performances less well than KNN model. Therefore, it's better to use KNN model for the purpose of prediction.

To sum up, we can easily distinguish the profile of institutions of each Control type. For prediction, KNN model is a better choice. For presentation or interpretation, Decision Tree is easier to understand.

#6 Future Studies]<a id="futurestudies"></a>

One question we would like to address in the future is to find a better algorithms to perform parameters tunning. There are many parameters for Decision Tree Model and the Grid Search algorithm in ```caret``` can only allow us to tune  ```cp``` parameter. In Python Grid Search allows us to test a combination of many parameters of the Decision Tree model at the same time, so it would be greate if we can find a counterpart in R.

Also, we would like to try out other model including Random Forest Tree, Neural Network, Support Vector Machine, etc to see if any other model performs better than two models we built in this analysis. 
